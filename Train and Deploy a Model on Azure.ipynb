{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/tutorials/img-classification-part1-training.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train and Deploy a Model on Azure Machine Learning service.\n\nIn this lab, you train the classifcation machine learning model on remote compute resources. You'll use the training and deployment workflow for Azure Machine Learning service (preview) in a Python Jupyter notebook.  \n\nLearn how to:\n\n> * Set up your development environment\n> * Access and examine the data\n> * Train a simple classification model on a remote cluster\n> * Review training results, find and register the best model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Set up your development environment\n\nAll the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n\n* Create an Experiment in an existing Workspace.\n* Configure AutoML using AutoMLConfig.\n* Importing Python packages\n* Connecting to a workspace to enable communication between your local computer and remote resources\n* Creating an experiment to track all your runs\n* Creating a remote compute target to use for training\n\n### Import packages\n\nImport Python packages you need in this session. Also display the Azure Machine Learning SDK version."
    },
    {
      "metadata": {
        "tags": [
          "check version"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Azure ML SDK Version:  1.0.72\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create the AMLS workspace...\n\n## You will be asked to log into Azure and be given a code in the output message area to enter."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\nws = Workspace.create(name='docs-ws',\n            subscription_id='c958680c-dc7a-403c-bb83-74f48dce46b3', \n            resource_group='docs-aml',\n            create_resource_group = True,\n            location='West US'\n            )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nprint(azureml.core.VERSION)\n\nfrom azureml.core import Workspace\nws = Workspace.get(name='docs-ws',\n            subscription_id='c958680c-dc7a-403c-bb83-74f48dce46b3', \n            resource_group='docs-aml')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.0.72\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Workspace Properties from Above:\n\nName | Description\n---- | -----------\nname            | A name you chose to call the workspace.  We'll use the value aready in the code.\nsubscription_id | The id of the subscription the workspace will be assigned to.  You can get this from the Azure portal.\nresource_group  | A name you want all the Azure resourced creates for the workspace to be associated with.  Makes rsource management easier.\nlocation | Azure data center location closest to you that support creation of AMLS workspaces.  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## To make reconnecting to this workspace in future notebooks easier, save the configuration setting using the code in the cell below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the configuration file.\nws.write_config(path='.', file_name='config.json')\nprint('Configuration saved.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Connect to workspace in future work...\n\nIn the future, we can use the code below to connect back to this workspace. \nThe code creates a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`.  You don't need to use this now since we are still connected from when we created the workspace but this will come in handy later."
    },
    {
      "metadata": {
        "tags": [
          "load workspace"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "# load workspace configuration from the config.json file in the current folder.\nws = Workspace.from_config()\nprint(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "docs-ws\twestus\tdocs-aml\twestus\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create experiment\n\nCreate an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
    },
    {
      "metadata": {
        "tags": [
          "create experiment"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'dp100labexperiment'\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)\nprint('Experiment created.')",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Experiment created.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create or Attach existing compute resource\nBy using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n\n**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
    },
    {
      "metadata": {
        "tags": [
          "create mlc",
          "amlcompute"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n\n# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D4_V2\")\n\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n                                                                min_nodes = compute_min_nodes, \n                                                                max_nodes = compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n     # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "found compute target. just use it. cpucluster\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You now have the necessary packages and compute resources to train a model in the cloud. \n\n## Verify you have the data\n\nYou already explored the data in the last lab.  You need to copy the data into the cloud so it can be accessed by your cloud training environment.  We saved the model training data to a csv file so all we have to do is load it."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Display a few rows of data to make sure the load worked."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_features = pd.read_csv(r'./BikeFeatures.csv', sep=';')",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_features.head()",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BikeTypeCat</th>\n      <th>AgeBandCat</th>\n      <th>IncomeBandCat</th>\n      <th>MaritalStatusCat</th>\n      <th>OccupationCat</th>\n      <th>CommuteDistanceCat</th>\n      <th>GenderCat</th>\n      <th>ChildrenAtHomeCat</th>\n      <th>CountryRegionCodeCat</th>\n      <th>HouseOwnerFlagCat</th>\n      <th>EducationCat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   BikeTypeCat  AgeBandCat  IncomeBandCat  MaritalStatusCat  OccupationCat  \\\n0            1           5              1                 1              1   \n1            0           3              0                 1              2   \n2            0           5              0                 1              3   \n3            1           5              1                 0              1   \n4            0           3              1                 1              3   \n\n   CommuteDistanceCat  GenderCat  ChildrenAtHomeCat  CountryRegionCodeCat  \\\n0                   2          1                  0                     1   \n1                   0          0                  1                     3   \n2                   2          0                  0                     5   \n3                   1          1                  0                     5   \n4                   4          0                  0                     0   \n\n   HouseOwnerFlagCat  EducationCat  \n0                  1             0  \n1                  0             2  \n2                  1             2  \n3                  1             1  \n4                  0             0  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you have an idea of what these images look like and the expected prediction outcome.\n\n### Upload data to the cloud\n\nNow make the data accessible remotely by uploading that data from your local machine into Azure so it can be accessed for remote training. The datastore is a convenient construct associated with your workspace for you to upload/download data, and interact with it from your remote compute targets. It is backed by Azure blob storage account.\n\nThe data file is uploaded into a directory named `dp100lab` at the root of the datastore."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request\n\ndata_folder = os.getcwd() \n\nprint(data_folder)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/library/Lab2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "tags": [
          "use datastore"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\n\nds.upload(src_dir=data_folder, target_path='dp100lab', overwrite=True, show_progress=True)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "AzureBlob docswsstorage55a670162d5 azureml-blobstore-844a3b1a-23d9-438b-8e8a-160f34aad9b1\nUploading an estimated of 11 files\nUploading /home/nbuser/library/Lab2/.azureml/config.json\nUploading /home/nbuser/library/Lab2/AWBikeSales-Copy.csv\nUploading /home/nbuser/library/Lab2/BikeFeatures.csv\nUploading /home/nbuser/library/Lab2/BikeModelFeatures.csv\nUploading /home/nbuser/library/Lab2/Lab2_Introduction.md\nUploading /home/nbuser/library/Lab2/Starting_Lab2.ipynb\nUploading /home/nbuser/library/Lab2/Train and Deploy a Model on Azure.ipynb\nUploading /home/nbuser/library/Lab2/bikemodelenv.yml\nUploading /home/nbuser/library/Lab2/biketypemodel.pkl\nUploading /home/nbuser/library/Lab2/dp100lab/train.py\nUploading /home/nbuser/library/Lab2/score.py\nUploaded /home/nbuser/library/Lab2/score.py, 1 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/dp100lab/train.py, 2 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/bikemodelenv.yml, 3 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/Lab2_Introduction.md, 4 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/.azureml/config.json, 5 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/Train and Deploy a Model on Azure.ipynb, 6 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/BikeFeatures.csv, 7 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/Starting_Lab2.ipynb, 8 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/BikeModelFeatures.csv, 9 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/AWBikeSales-Copy.csv, 10 files out of an estimated total of 11\nUploaded /home/nbuser/library/Lab2/biketypemodel.pkl, 11 files out of an estimated total of 11\nUploaded 11 files\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_8902328254ab47eda8ac8b04bf9f9595"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train on a remote cluster\n\nFor this task, submit the job to the remote training cluster you set up earlier.  To submit a job you:\n* Create a directory\n* Create a training script\n* Create an estimator object\n* Submit the job \n\n### Create a directory\n\nCreate a directory to deliver the necessary code from your computer to the remote resource."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = os.path.join(os.getcwd(), \"dp100lab\")\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Confirm the df_feeatures one more time...\ndf_features.dtypes",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "BikeTypeCat             int64\nAgeBandCat              int64\nIncomeBandCat           int64\nMaritalStatusCat        int64\nOccupationCat           int64\nCommuteDistanceCat      int64\nGenderCat               int64\nChildrenAtHomeCat       int64\nCountryRegionCodeCat    int64\nHouseOwnerFlagCat       int64\nEducationCat            int64\ndtype: object"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a training script\n\nTo submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## About training scripts...\n\n#### To train a model in an Azure container, we need to get the model training script to the container. We start by saving the model training script to a Python script file, i.e. .py.  This will be uploaded to the Azure container later.  We don't need any exploratory code in this script, just what is needed to train the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Clasiification training script below.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.externals import joblib\n\nfrom azureml.core import Run\nprint('Libraries Imported')\n\n# ***  Azure Machine Learning service specfic code starts... ***\n\n# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--maxdepth', type=float, dest='max_depth', default=14, help='max_depth')\nargs = parser.parse_args()\n\n\ndata_folder = args.data_folder\nmax_depth = args.max_depth\n\nprint('Data folder:', data_folder)\n\n# get hold of the current run\nrun = Run.get_context()\n\n# ***  Azure Machine Learning service specfic code ends. ***\n\n# filepath = data_folder + '/BikeModelFeatures.csv'\nfilepath = os.path.join(data_folder, 'BikeFeatures.csv')\n\ndf_features = pd.read_csv(filepath, sep=';')\n\n# load train and test set into numpy arrays\n\nX_train_all , X_test_all = train_test_split(df_features.values,test_size=0.2)       #test_size=0.5(whole_data)\n\n# Column 0 has the value we want to predict\nX_train_all[:,0]\n\ny_train = X_train_all[:,0]\n\ny_test = X_test_all[:,0]\n\nX_train_all.shape\n\nX_train = X_train_all[:,1:11]\nX_train.shape\n\nX_test = X_test_all[:,1:11]\nX_test\n\n# Set the max_depth model hyperparameter to = max_depth which is the parameter value we created in the Azure ML service specific code above, i.e. , max_depth = max_depth \n# print('trainng RandomForestClassifier...')\n#classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42, max_depth = max_depth)\nclassifier =  AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=10, max_depth = 20), n_estimators= 100, learning_rate=0.2)\nclassifier.fit(X_train, y_train)\nprint('Classifier Model Trained.')\n\n\n\n# Predict using the test data...\nprint('Running the test dataset through...')\ny_predtest = classifier.predict(X_test)\nprint('Test dataset scored.')\n\n# calculate accuracy on the prediction\nacc = np.average(y_predtest == y_test)\nprint('Accuracy is', acc)\n\n\n# ***  Azure Machine Learning service specfic code starts... ***\nrun.log('data_dir', data_folder)\nrun.log('accuracy', np.float(acc))\n\nos.makedirs('outputs', exist_ok=True)\n\n# note file saved in the outputs folder is automatically uploaded into experiment record\njoblib.dump(value=classifier, filename='outputs/biketypemodel.pkl')\n\n# ***  Azure Machine Learning service specfic code ends. ***",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting /home/nbuser/library/Lab2/dp100lab/train.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice how the script gets data and saves models:\n\n+ The training script reads an argument to find the directory containing the data.  When you submit the job later, you point to the datastore for this argument:\n`parser.add_argument('--data-folder', type=str, dest='data_folder', help='data directory mounting point')`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n+ The training script saves your model into a directory named outputs. <br/>\n`joblib.dump(value=clf, filename='outputs/biketypemodel.pkl')`<br/>\nAnything written in this directory is automatically uploaded into your workspace. You'll access your model from this directory later in the tutorial."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create an estimator\n\nAn estimator object is used to submit the run.  Create your estimator by running the following code to define:\n\n* The name of the estimator object, `est`\n* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n* The compute target.  In this case you will use the AmlCompute you created\n* The training script name, train.py\n* Parameters required from the training script \n* Python packages needed for training\n\nIn this tutorial, this target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the datastore (`ds.path('dp100lab').as_mount()`)."
    },
    {
      "metadata": {
        "tags": [
          "configure estimator"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.path('dp100lab').as_mount(),\n    '--maxdepth': 12\n}\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script='train.py',\n                conda_packages=['scikit-learn','pandas'])\n\nprint('Executed')",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Executed\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "est  # take a look at the est object...",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "<azureml.train.estimator._estimator.Estimator at 0x7fc27a3eb828>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=5, \n                                               tags={\"data\": \"Bike Features\",  \"method\" : \"sklearn\"}, \n                                               description='Classification model sklearn')",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Submit the job to the cluster\n\nRun the experiment by submitting the estimator object. And you can navigate to Azure portal to monitor the run."
    },
    {
      "metadata": {
        "tags": [
          "remote run",
          "amlcompute",
          "scikit-learn"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(config=est)\nrun",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>dp100labexperiment</td><td>dp100labexperiment_1581021317_d11ff3ea</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/dp100labexperiment/runs/dp100labexperiment_1581021317_d11ff3ea?wsid=/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourcegroups/docs-aml/workspaces/docs-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: dp100labexperiment,\nId: dp100labexperiment_1581021317_d11ff3ea,\nType: azureml.scriptrun,\nStatus: Starting)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  We can see what built-in datastores are available with this code below...\ndatastores = ws.datastores\nfor name, ds in datastores.items():\n    print(name, ds.datastore_type)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "workspaceblobstore AzureBlob\nworkspacefilestore AzureFile\nazureml_globaldatasets AzureBlob\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n\n## Monitor a remote run\n\nIn total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies (`conda_packages` parameter in the above estimator constructor) don't change, the same image is reused and hence the container start up time is much faster.\n\nHere is what's happening while you wait:\n\n- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n\n  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n\n- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n\n- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n\n- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n\n\nYou can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n\n### Jupyter widget\n\nWatch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
    },
    {
      "metadata": {
        "tags": [
          "use notebook widget"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43d674763425427da871885f105b7f0b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/dp100labexperiment/runs/dp100labexperiment_1581021317_d11ff3ea?wsid=/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourcegroups/docs-aml/workspaces/docs-ws\", \"run_id\": \"dp100labexperiment_1581021317_d11ff3ea\", \"run_properties\": {\"run_id\": \"dp100labexperiment_1581021317_d11ff3ea\", \"created_utc\": \"2020-02-06T20:35:27.576951Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"1f8e961a-66c6-4d03-8656-b15633676aa1\", \"azureml.git.repository_uri\": \"https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio\", \"mlflow.source.git.repoURL\": \"https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"3eb151d4f1ee7740657be31dabcc8179c030b804\", \"mlflow.source.git.commit\": \"3eb151d4f1ee7740657be31dabcc8179c030b804\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_fd0e07448b33ff52d674b873ca7dbcdd\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"resizing\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":1,\\\"CurrentNodeCount\\\":3}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-02-06T20:37:32.903117Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/55_azureml-execution-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt?sv=2019-02-02&sr=b&sig=itl%2BBfvbB7hmane4lohYfA0213XvVIHSn%2BIQ7%2FthXqE%3D&st=2020-02-06T20%3A27%3A48Z&se=2020-02-07T04%3A37%3A48Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/65_job_prep-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt?sv=2019-02-02&sr=b&sig=V5dSYq85mzuG1kUUE8bM6FSdoQSJEPVuBVcbNUBaXQM%3D&st=2020-02-06T20%3A27%3A48Z&se=2020-02-07T04%3A37%3A48Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=yPsuPjzTPW83s5cBWiGs7MtSDtnsbPGMOtKEs2vstPY%3D&st=2020-02-06T20%3A27%3A49Z&se=2020-02-07T04%3A37%3A49Z&sp=r\", \"azureml-logs/75_job_post-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/75_job_post-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt?sv=2019-02-02&sr=b&sig=DR4ZCmnSszax%2FHTzJn0yrzLFLOzEaPdri2m8ro7hxBI%3D&st=2020-02-06T20%3A27%3A49Z&se=2020-02-07T04%3A37%3A49Z&sp=r\", \"azureml-logs/process_info.json\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=8MRiNzOFXKUlP9GDFWCwI9t6DHSuA1RLBpHQw90aoHc%3D&st=2020-02-06T20%3A27%3A49Z&se=2020-02-07T04%3A37%3A49Z&sp=r\", \"azureml-logs/process_status.json\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=1o2R7y2qvvGiGaCJZeAAWI%2FwLu%2B4TO0dATYnBw2K1sw%3D&st=2020-02-06T20%3A27%3A49Z&se=2020-02-07T04%3A37%3A49Z&sp=r\", \"logs/azureml/156_azureml.log\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/logs/azureml/156_azureml.log?sv=2019-02-02&sr=b&sig=ylS9NqznZqTuOPdB7E%2FVh1RpQr9r3L42rlU8D%2BGz9e8%3D&st=2020-02-06T20%3A27%3A48Z&se=2020-02-07T04%3A37%3A48Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=BXU9J6v47VmCpKrABbaNsPLV6qm6ySI6PiNXpFCRuuE%3D&st=2020-02-06T20%3A27%3A49Z&se=2020-02-07T04%3A37%3A49Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=ugupfOY8mHM%2FOsaipalb7lQ%2BgLKO1sbxCSq%2B3hZop08%3D&st=2020-02-06T20%3A27%3A49Z&se=2020-02-07T04%3A37%3A49Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt\"], [\"logs/azureml/156_azureml.log\"]], \"run_duration\": \"0:02:05\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"data_dir\", \"run_id\": \"dp100labexperiment_1581021317_d11ff3ea\", \"categories\": [0], \"series\": [{\"data\": [\"/mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/dp100lab\"]}]}, {\"name\": \"accuracy\", \"run_id\": \"dp100labexperiment_1581021317_d11ff3ea\", \"categories\": [0], \"series\": [{\"data\": [0.47155540940480106]}]}], \"run_logs\": \"2020-02-06 20:37:04,393|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2020-02-06 20:37:04,394|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2020-02-06 20:37:04,400|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2020-02-06 20:37:04,400|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-02-06 20:37:04,670|azureml._base_sdk_common.user_agent|DEBUG|Fetching client info from /root/.azureml/clientinfo.json\\n2020-02-06 20:37:04,671|azureml._base_sdk_common.user_agent|DEBUG|Error loading client info: [Errno 2] No such file or directory: '/root/.azureml/clientinfo.json'\\n2020-02-06 20:37:05,102|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7fb787109268> for run source azureml.scriptrun\\n2020-02-06 20:37:05,104|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-02-06 20:37:05,113|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:05,115|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-02-06 20:37:05,115|azureml.core.authentication|DEBUG|Time to expire 1814301.884606 seconds\\n2020-02-06 20:37:05,116|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,116|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,116|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,116|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,116|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,116|azureml._base_sdk_common.service_discovery|DEBUG|Constructing mms service url in from history url environment variable None, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,117|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,117|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,117|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,244|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:05,251|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:05,263|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:05,270|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:05,277|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:05,284|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:05,284|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-02-06 20:37:05,285|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-06 20:37:05,285|msrest.http_logger|DEBUG|Request URL: 'https://westus.experiments.azureml.net/history/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runs/dp100labexperiment_1581021317_d11ff3ea'\\n2020-02-06 20:37:05,286|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-02-06 20:37:05,286|msrest.http_logger|DEBUG|Request headers:\\n2020-02-06 20:37:05,287|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-02-06 20:37:05,287|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-06 20:37:05,287|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '85508c2f-ec57-4d62-ac8e-d59399e7ff44'\\n2020-02-06 20:37:05,287|msrest.http_logger|DEBUG|    'request-id': '85508c2f-ec57-4d62-ac8e-d59399e7ff44'\\n2020-02-06 20:37:05,288|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.11 azureml._restclient/core.1.0.85'\\n2020-02-06 20:37:05,288|msrest.http_logger|DEBUG|Request body:\\n2020-02-06 20:37:05,288|msrest.http_logger|DEBUG|None\\n2020-02-06 20:37:05,288|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-06 20:37:05,288|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-06 20:37:05,288|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-06 20:37:05,289|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-06 20:37:05,549|msrest.http_logger|DEBUG|Response status: 200\\n2020-02-06 20:37:05,549|msrest.http_logger|DEBUG|Response headers:\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'Date': 'Thu, 06 Feb 2020 20:37:05 GMT'\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-02-06 20:37:05,550|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '85508c2f-ec57-4d62-ac8e-d59399e7ff44'\\n2020-02-06 20:37:05,551|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-02-06 20:37:05,551|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-02-06 20:37:05,551|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-02-06 20:37:05,551|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-02-06 20:37:05,551|msrest.http_logger|DEBUG|Response content:\\n2020-02-06 20:37:05,552|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 26,\\n  \\\"rootRunId\\\": \\\"dp100labexperiment_1581021317_d11ff3ea\\\",\\n  \\\"experimentId\\\": \\\"d4779073-df21-4813-8377-9b40e4699003\\\",\\n  \\\"createdUtc\\\": \\\"2020-02-06T20:35:27.5769515+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"a416a4d3-ce1f-49fd-ba62-64c169eb5b9b\\\",\\n    \\\"userPuId\\\": \\\"10032000522E5EE2\\\",\\n    \\\"userIdp\\\": \\\"live.com\\\",\\n    \\\"userAltSecId\\\": \\\"1:live.com:00030000C75637BF\\\",\\n    \\\"userIss\\\": \\\"https://sts.windows.net/444864cd-9504-43a6-8eea-1f5aa64c79a8/\\\",\\n    \\\"userTenantId\\\": \\\"444864cd-9504-43a6-8eea-1f5aa64c79a8\\\",\\n    \\\"userName\\\": \\\"Alejandra Taborda\\\"\\n  },\\n  \\\"userId\\\": \\\"a416a4d3-ce1f-49fd-ba62-64c169eb5b9b\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 9,\\n  \\\"runUuid\\\": \\\"b76921ed-a824-49b2-9d35-2c89e5478e63\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"b76921ed-a824-49b2-9d35-2c89e5478e63\\\",\\n  \\\"runId\\\": \\\"dp100labexperiment_1581021317_d11ff3ea\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-02-06T20:36:43.5648564+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.dp100labexperiment_1581021317_d11ff3ea\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"1f8e961a-66c6-4d03-8656-b15633676aa1\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio\\\",\\n    \\\"azureml.git.branch\\\": \\\"master\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"master\\\",\\n    \\\"azureml.git.commit\\\": \\\"3eb151d4f1ee7740657be31dabcc8179c030b804\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"3eb151d4f1ee7740657be31dabcc8179c030b804\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"AzureML.DerivedImageName\\\": \\\"azureml/azureml_fd0e07448b33ff52d674b873ca7dbcdd\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"train.py\\\",\\n  \\\"target\\\": \\\"cpucluster\\\",\\n  \\\"tags\\\": {\\n    \\\"_aml_system_ComputeTargetStatus\\\": \\\"{\\\\\\\"AllocationState\\\\\\\":\\\\\\\"resizing\\\\\\\",\\\\\\\"PreparingNodeCount\\\\\\\":0,\\\\\\\"RunningNodeCount\\\\\\\":1,\\\\\\\"CurrentNodeCount\\\\\\\":3}\\\"\\n  },\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://westus.experiments.azureml.net/execution/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runId/dp100labexperiment_1581021317_d11ff3ea/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://westus.experiments.azureml.net/execution/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runId/dp100labexperiment_1581021317_d11ff3ea/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-02-06 20:37:05,560|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-02-06 20:37:05,561|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '1f8e961a-66c6-4d03-8656-b15633676aa1', 'azureml.git.repository_uri': 'https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio', 'mlflow.source.git.repoURL': 'https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '3eb151d4f1ee7740657be31dabcc8179c030b804', 'mlflow.source.git.commit': '3eb151d4f1ee7740657be31dabcc8179c030b804', 'azureml.git.dirty': 'True', 'AzureML.DerivedImageName': 'azureml/azureml_fd0e07448b33ff52d674b873ca7dbcdd', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-02-06 20:37:05,561|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-02-06 20:37:05,561|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2020-02-06 20:37:05,562|azureml.WorkerPool|DEBUG|[START]\\n2020-02-06 20:37:05,562|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-02-06 20:37:05,562|azureml.RunStatusContext|DEBUG|[START]\\n2020-02-06 20:37:05,562|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-02-06 20:37:05,562|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-02-06 20:37:05,562|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-02-06 20:37:05,563|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/azureml/dp100labexperiment_1581021317_d11ff3ea\\n2020-02-06 20:37:05,563|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-02-06 20:37:05,563|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/azureml/dp100labexperiment_1581021317_d11ff3ea\\n2020-02-06 20:37:07,565|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,565|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,565|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,565|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,566|azureml._base_sdk_common.service_discovery|DEBUG|Constructing mms service url in from history url environment variable None, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,567|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westus.experiments.azureml.net.\\n2020-02-06 20:37:07,574|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:07,575|azureml._run_impl.run_history_facade|DEBUG|Created a static thread pool for RunHistoryFacade class\\n2020-02-06 20:37:07,582|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:07,590|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:07,598|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:07,605|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:07,606|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-02-06 20:37:07,606|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|Request URL: 'https://westus.experiments.azureml.net/history/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runs/dp100labexperiment_1581021317_d11ff3ea'\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|Request headers:\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '7d5f9d64-135f-4d92-9ca7-5136dff564d3'\\n2020-02-06 20:37:07,607|msrest.http_logger|DEBUG|    'request-id': '7d5f9d64-135f-4d92-9ca7-5136dff564d3'\\n2020-02-06 20:37:07,608|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.11 azureml._restclient/core.1.0.85'\\n2020-02-06 20:37:07,608|msrest.http_logger|DEBUG|Request body:\\n2020-02-06 20:37:07,608|msrest.http_logger|DEBUG|None\\n2020-02-06 20:37:07,608|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-06 20:37:07,608|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-06 20:37:07,608|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-06 20:37:07,608|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-06 20:37:07,868|msrest.http_logger|DEBUG|Response status: 200\\n2020-02-06 20:37:07,869|msrest.http_logger|DEBUG|Response headers:\\n2020-02-06 20:37:07,869|msrest.http_logger|DEBUG|    'Date': 'Thu, 06 Feb 2020 20:37:07 GMT'\\n2020-02-06 20:37:07,869|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-06 20:37:07,869|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-02-06 20:37:07,869|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-02-06 20:37:07,869|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-02-06 20:37:07,870|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-02-06 20:37:07,870|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '7d5f9d64-135f-4d92-9ca7-5136dff564d3'\\n2020-02-06 20:37:07,870|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-02-06 20:37:07,870|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-02-06 20:37:07,870|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-02-06 20:37:07,870|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-02-06 20:37:07,871|msrest.http_logger|DEBUG|Response content:\\n2020-02-06 20:37:07,871|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 26,\\n  \\\"rootRunId\\\": \\\"dp100labexperiment_1581021317_d11ff3ea\\\",\\n  \\\"experimentId\\\": \\\"d4779073-df21-4813-8377-9b40e4699003\\\",\\n  \\\"createdUtc\\\": \\\"2020-02-06T20:35:27.5769515+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"a416a4d3-ce1f-49fd-ba62-64c169eb5b9b\\\",\\n    \\\"userPuId\\\": \\\"10032000522E5EE2\\\",\\n    \\\"userIdp\\\": \\\"live.com\\\",\\n    \\\"userAltSecId\\\": \\\"1:live.com:00030000C75637BF\\\",\\n    \\\"userIss\\\": \\\"https://sts.windows.net/444864cd-9504-43a6-8eea-1f5aa64c79a8/\\\",\\n    \\\"userTenantId\\\": \\\"444864cd-9504-43a6-8eea-1f5aa64c79a8\\\",\\n    \\\"userName\\\": \\\"Alejandra Taborda\\\"\\n  },\\n  \\\"userId\\\": \\\"a416a4d3-ce1f-49fd-ba62-64c169eb5b9b\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 9,\\n  \\\"runUuid\\\": \\\"b76921ed-a824-49b2-9d35-2c89e5478e63\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"b76921ed-a824-49b2-9d35-2c89e5478e63\\\",\\n  \\\"runId\\\": \\\"dp100labexperiment_1581021317_d11ff3ea\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-02-06T20:36:43.5648564+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.dp100labexperiment_1581021317_d11ff3ea\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"1f8e961a-66c6-4d03-8656-b15633676aa1\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio\\\",\\n    \\\"azureml.git.branch\\\": \\\"master\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"master\\\",\\n    \\\"azureml.git.commit\\\": \\\"3eb151d4f1ee7740657be31dabcc8179c030b804\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"3eb151d4f1ee7740657be31dabcc8179c030b804\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"AzureML.DerivedImageName\\\": \\\"azureml/azureml_fd0e07448b33ff52d674b873ca7dbcdd\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"train.py\\\",\\n  \\\"target\\\": \\\"cpucluster\\\",\\n  \\\"tags\\\": {\\n    \\\"_aml_system_ComputeTargetStatus\\\": \\\"{\\\\\\\"AllocationState\\\\\\\":\\\\\\\"resizing\\\\\\\",\\\\\\\"PreparingNodeCount\\\\\\\":0,\\\\\\\"RunningNodeCount\\\\\\\":1,\\\\\\\"CurrentNodeCount\\\\\\\":3}\\\"\\n  },\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://westus.experiments.azureml.net/execution/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runId/dp100labexperiment_1581021317_d11ff3ea/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://westus.experiments.azureml.net/execution/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runId/dp100labexperiment_1581021317_d11ff3ea/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-02-06 20:37:07,875|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-02-06 20:37:07,876|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '1f8e961a-66c6-4d03-8656-b15633676aa1', 'azureml.git.repository_uri': 'https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio', 'mlflow.source.git.repoURL': 'https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '3eb151d4f1ee7740657be31dabcc8179c030b804', 'mlflow.source.git.commit': '3eb151d4f1ee7740657be31dabcc8179c030b804', 'azureml.git.dirty': 'True', 'AzureML.DerivedImageName': 'azureml/azureml_fd0e07448b33ff52d674b873ca7dbcdd', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-02-06 20:37:07,876|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-02-06 20:37:10,618|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-02-06 20:37:10,618|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-02-06 20:37:10,619|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-02-06 20:37:11,012|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-02-06 20:37:11,012|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/azureml/dp100labexperiment_1581021317_d11ff3ea\\n2020-02-06 20:37:11,013|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/azureml/dp100labexperiment_1581021317_d11ff3ea to /mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/azureml/dp100labexperiment_1581021317_d11ff3ea\\n2020-02-06 20:37:11,013|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/azureml/dp100labexperiment_1581021317_d11ff3ea\\n2020-02-06 20:37:11,013|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-02-06 20:37:11,013|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-02-06 20:37:11,013|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea|INFO|complete is not setting status for submitted runs.\\n2020-02-06 20:37:11,013|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-02-06 20:37:11,013|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-02-06 20:37:11,014|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-02-06 20:37:11,014|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-02-06 20:37:11,014|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-02-06 20:37:11,014|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-02-06 20:37:11,015|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-02-06 20:37:11,015|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-02-06 20:37:11,015|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-02-06 20:37:11,015|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-02-06 20:37:11,015|azureml.RunStatusContext|DEBUG|[STOP]\\n2020-02-06 20:37:11,015|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-02-06 20:37:11,016|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-02-06 20:37:11,017|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Start]\\n2020-02-06 20:37:11,017|azureml.BatchTaskQueueAdd_1_Batches.WorkerPool|DEBUG|submitting future: _handle_batch\\n2020-02-06 20:37:11,017|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|Batch size 2.\\n2020-02-06 20:37:11,017|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch|DEBUG|Using basic handler - no exception handling\\n2020-02-06 20:37:11,017|azureml._restclient.clientbase.WorkerPool|DEBUG|submitting future: _log_batch\\n2020-02-06 20:37:11,018|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Adding task 0__handle_batch to queue of approximate size: 0\\n2020-02-06 20:37:11,019|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.post_batch-async:False|DEBUG|[START]\\n2020-02-06 20:37:11,019|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch|DEBUG|Using basic handler - no exception handling\\n2020-02-06 20:37:11,019|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Stop] - waiting default timeout\\n2020-02-06 20:37:11,021|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-06 20:37:11,021|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|Adding task 0__log_batch to queue of approximate size: 0\\n2020-02-06 20:37:11,021|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[START]\\n2020-02-06 20:37:11,022|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-06 20:37:11,022|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Overriding default flush timeout from None to 120\\n2020-02-06 20:37:11,023|msrest.http_logger|DEBUG|Request URL: 'https://westus.experiments.azureml.net/history/v1.0/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/dp100labexperiment/runs/dp100labexperiment_1581021317_d11ff3ea/batch/metrics'\\n2020-02-06 20:37:11,023|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0__handle_batch)].\\n2020-02-06 20:37:11,023|msrest.http_logger|DEBUG|Request method: 'POST'\\n2020-02-06 20:37:11,023|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[START]\\n2020-02-06 20:37:11,023|msrest.http_logger|DEBUG|Request headers:\\n2020-02-06 20:37:11,024|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|Awaiter is BatchTaskQueueAdd_1_Batches\\n2020-02-06 20:37:11,024|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-02-06 20:37:11,024|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[STOP]\\n2020-02-06 20:37:11,024|msrest.http_logger|DEBUG|    'Content-Type': 'application/json-patch+json; charset=utf-8'\\n2020-02-06 20:37:11,024|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|\\n2020-02-06 20:37:11,025|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '268f9440-3ce9-4359-a872-5b4ee2c0bbdd'\\n2020-02-06 20:37:11,025|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[STOP]\\n2020-02-06 20:37:11,025|msrest.http_logger|DEBUG|    'request-id': '268f9440-3ce9-4359-a872-5b4ee2c0bbdd'\\n2020-02-06 20:37:11,025|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-02-06 20:37:11,025|msrest.http_logger|DEBUG|    'Content-Length': '816'\\n2020-02-06 20:37:11,026|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-02-06 20:37:11,026|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.11 azureml._restclient/core.1.0.85 sdk_run'\\n2020-02-06 20:37:11,026|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [AsyncTask(0__log_batch)].\\n2020-02-06 20:37:11,026|msrest.http_logger|DEBUG|Request body:\\n2020-02-06 20:37:11,027|msrest.http_logger|DEBUG|{\\\"values\\\": [{\\\"metricId\\\": \\\"006cfd47-cf08-4e44-b205-eeaf7fe04b71\\\", \\\"metricType\\\": \\\"azureml.v1.scalar\\\", \\\"createdUtc\\\": \\\"2020-02-06T20:37:10.618043Z\\\", \\\"name\\\": \\\"data_dir\\\", \\\"description\\\": \\\"\\\", \\\"numCells\\\": 1, \\\"cells\\\": [{\\\"data_dir\\\": \\\"/mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/dp100lab\\\"}], \\\"schema\\\": {\\\"numProperties\\\": 1, \\\"properties\\\": [{\\\"propertyId\\\": \\\"data_dir\\\", \\\"name\\\": \\\"data_dir\\\", \\\"type\\\": \\\"string\\\"}]}}, {\\\"metricId\\\": \\\"0d858595-d170-4924-b83e-171e3b3ca5fd\\\", \\\"metricType\\\": \\\"azureml.v1.scalar\\\", \\\"createdUtc\\\": \\\"2020-02-06T20:37:10.619339Z\\\", \\\"name\\\": \\\"accuracy\\\", \\\"description\\\": \\\"\\\", \\\"numCells\\\": 1, \\\"cells\\\": [{\\\"accuracy\\\": 0.47155540940480106}], \\\"schema\\\": {\\\"numProperties\\\": 1, \\\"properties\\\": [{\\\"propertyId\\\": \\\"accuracy\\\", \\\"name\\\": \\\"accuracy\\\", \\\"type\\\": \\\"float\\\"}]}}]}\\n2020-02-06 20:37:11,027|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-06 20:37:11,027|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-06 20:37:11,027|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-06 20:37:11,027|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-06 20:37:11,329|msrest.http_logger|DEBUG|Response status: 200\\n2020-02-06 20:37:11,329|msrest.http_logger|DEBUG|Response headers:\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'Date': 'Thu, 06 Feb 2020 20:37:11 GMT'\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'Content-Length': '0'\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '268f9440-3ce9-4359-a872-5b4ee2c0bbdd'\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-02-06 20:37:11,330|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-02-06 20:37:11,331|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-02-06 20:37:11,331|msrest.http_logger|DEBUG|Response content:\\n2020-02-06 20:37:11,331|msrest.http_logger|DEBUG|\\n2020-02-06 20:37:11,332|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.post_batch-async:False|DEBUG|[STOP]\\n2020-02-06 20:37:11,527|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch.WaitingTask|DEBUG|[START]\\n2020-02-06 20:37:11,528|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch.WaitingTask|DEBUG|Awaiter is PostMetricsBatch\\n2020-02-06 20:37:11,528|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch.WaitingTask|DEBUG|[STOP]\\n2020-02-06 20:37:11,528|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|Waiting on task: 0__log_batch.\\n1 tasks left. Current duration of flush 0.00043272972106933594 seconds.\\nWaiting on task: 0__log_batch.\\n1 tasks left. Current duration of flush 0.25084710121154785 seconds.\\n\\n2020-02-06 20:37:11,528|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-02-06 20:37:11,528|azureml._SubmittedRun#dp100labexperiment_1581021317_d11ff3ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-02-06 20:37:11,528|azureml.SendRunKillSignal|DEBUG|[STOP]\\n2020-02-06 20:37:11,529|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[START]\\n2020-02-06 20:37:11,529|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[STOP]\\n2020-02-06 20:37:11,529|azureml.WorkerPool|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.72\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "By the way, if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get log results upon completion\n\nModel training happens in the background. You can use `wait_for_completion` to block and wait until the model has completed training before running more code. "
    },
    {
      "metadata": {
        "tags": [
          "remote run",
          "amlcompute",
          "scikit-learn"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "# specify show_output to True for a verbose log\nrun.wait_for_completion(show_output=False) ",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "{'runId': 'dp100labexperiment_1581021317_d11ff3ea',\n 'target': 'cpucluster',\n 'status': 'Completed',\n 'startTimeUtc': '2020-02-06T20:36:43.564856Z',\n 'endTimeUtc': '2020-02-06T20:37:32.903117Z',\n 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n  'ContentSnapshotId': '1f8e961a-66c6-4d03-8656-b15633676aa1',\n  'azureml.git.repository_uri': 'https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio',\n  'mlflow.source.git.repoURL': 'https://github.com/MicrosoftLearning/DP-100-Designing-and-Implementing-a-Data-Science-Solutio',\n  'azureml.git.branch': 'master',\n  'mlflow.source.git.branch': 'master',\n  'azureml.git.commit': '3eb151d4f1ee7740657be31dabcc8179c030b804',\n  'mlflow.source.git.commit': '3eb151d4f1ee7740657be31dabcc8179c030b804',\n  'azureml.git.dirty': 'True',\n  'AzureML.DerivedImageName': 'azureml/azureml_fd0e07448b33ff52d674b873ca7dbcdd',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n 'inputDatasets': [],\n 'runDefinition': {'script': 'train.py',\n  'useAbsolutePath': False,\n  'arguments': ['--data-folder',\n   '$AZUREML_DATAREFERENCE_650b18eafdc2410e82ad2cc134d24f99',\n   '--maxdepth',\n   '12'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'cpucluster',\n  'dataReferences': {'650b18eafdc2410e82ad2cc134d24f99': {'dataStoreName': 'workspaceblobstore',\n    'mode': 'Mount',\n    'pathOnDataStore': 'dp100lab',\n    'pathOnCompute': None,\n    'overwrite': False}},\n  'data': {},\n  'jobName': None,\n  'maxRunDurationSeconds': None,\n  'nodeCount': 1,\n  'environment': {'name': 'Experiment dp100labexperiment Environment',\n   'version': 'Autosave_2020-02-06T16:02:24Z_22c4b351',\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'channels': ['conda-forge'],\n     'dependencies': ['python=3.6.2',\n      {'pip': ['azureml-defaults']},\n      'scikit-learn',\n      'pandas'],\n     'name': 'azureml_9a645593c8f62f6c5223f40cca3b6b63'},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': True,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': 1},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': True,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []}},\n 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/55_azureml-execution-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt?sv=2019-02-02&sr=b&sig=tY0DE%2FhiSXxTxiCN7fKXdAStyhtfJX0vdo7x8yoDaIo%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'azureml-logs/65_job_prep-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/65_job_prep-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt?sv=2019-02-02&sr=b&sig=6YB8vY%2BthZoblnculEF6zdwTAbfyQ34bfQY24yIKeRI%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'azureml-logs/70_driver_log.txt': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=BIpkb%2BgKEqflRcjav0%2B9BKLe7cEOziG%2B1mL0SMYeboI%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'azureml-logs/75_job_post-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/75_job_post-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt?sv=2019-02-02&sr=b&sig=%2BZPgmdiYaxvas%2BWkKKaD%2F5bB%2FAyftYlBGGsPKiktO2Q%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'azureml-logs/process_info.json': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=TXiVbjblOPp2tpyl5ZLfwN3ZXnA5gRgWr%2Bke81GKK0M%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'azureml-logs/process_status.json': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=hM%2FAPKgklMwYr7vWrknkhAlhueA3HOjfsIcsYjiNCbE%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'logs/azureml/156_azureml.log': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/logs/azureml/156_azureml.log?sv=2019-02-02&sr=b&sig=iSQMUB9WqEHQ%2BnS8Xwp44%2FhoMR%2FZiTR2XAmF3%2FnBICU%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'logs/azureml/job_prep_azureml.log': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=z2HYE%2FkDNsaoMRzYPEg6RgL45L2nPuNBWbppMNlDKoM%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r',\n  'logs/azureml/job_release_azureml.log': 'https://docswsstorage55a670162d5.blob.core.windows.net/azureml/ExperimentRun/dcid.dp100labexperiment_1581021317_d11ff3ea/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=Z72ZaEM7PffUPnF48CE7yC%2FQ1nSbADhbZ5%2BTtubOxYA%3D&st=2020-02-06T20%3A28%3A32Z&se=2020-02-07T04%3A38%3A32Z&sp=r'}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Display run results\n\nYou now have a model trained on a remote cluster.  Retrieve all the metrics logged during the run, including the accuracy of the model:"
    },
    {
      "metadata": {
        "tags": [
          "get metrics"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(run.get_metrics())",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'data_dir': '/mnt/batch/tasks/shared/LS_root/jobs/docs-ws/azureml/dp100labexperiment_1581021317_d11ff3ea/mounts/workspaceblobstore/dp100lab', 'accuracy': 0.47155540940480106}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the next tutorial you will explore this model in more detail.\n\n## Register model\n\nThe last step in the training script wrote the file `outputs/biketypemodel.pkl` in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n\nYou can see files associated with that run."
    },
    {
      "metadata": {
        "tags": [
          "query history"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(run.get_file_names())",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['azureml-logs/55_azureml-execution-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt', 'azureml-logs/65_job_prep-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_aabeb345776fb00498dfe5c8d1ec177ba840ae93f59099d20db1fbdc4b287c9a_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/156_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log', 'outputs/biketypemodel.pkl']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
    },
    {
      "metadata": {
        "tags": [
          "register model from history"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "# register model \nmodel = run.register_model(model_name='biketypemodel', model_path='outputs/biketypemodel.pkl')\nprint(model.name, model.id, model.version, sep='\\t')",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": "biketypemodel\tbiketypemodel:4\t4\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "_______________________"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Lab 2: Part B Deploy the model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, you're ready to deploy the model as a web service in Azure Container Instances (ACI). A web service is an image, in this case a Docker image, that encapsulates the scoring logic and the model itself.\n\nIn this part of the lab, you use Azure Machine Learning service (Preview) to:\n\nSet up your testing environment\nRetrieve the model from your workspace\nTest the model locally\nDeploy the model to ACI\nTest the deployed model\n\nACI is not ideal for production deployments, but it is great for testing and understanding the workflow. For scalable production deployments, consider using AKS."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Retrieve the model\n\nYou registered a model in your workspace previously. Assuming you needed to come back later to deploy the model, load this workspace and download the model to your local directory."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nws = Workspace.from_config()\nmodel=Model(ws, 'biketypemodel')\nmodel.download(target_dir='.', exist_ok=True)\nimport os \n# verify the downloaded model file\nos.stat('./biketypemodel.pkl')\n\nprint('Complete')",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Complete\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deploy as web service\n\nOnce you've tested the model and are satisfied with the results, deploy the model as a web service hosted in ACI. \n\nTo build the correct environment for ACI, provide the following:\n* A scoring script to show how to use the model\n* An environment file to show what packages need to be installed\n* A configuration file to build the ACI\n* The model you trained before\n\n### Create scoring script\n\nCreate the scoring script, called score.py, used by the web service call to show how to use the model.\n\nYou must include two required functions into the scoring script:\n* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n\n* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score.py\nimport json\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\nfrom azureml.core.model import Model\n\ndef init():\n    global model\n    # retreive the path to the model file using the model name\n    model_path = Model.get_model_path('biketypemodel')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    # make prediction\n    y_hat = model.predict(data)\n    # you can return any data type as long as it is JSON-serializable\n    return y_hat.tolist()",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting score.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create environment file\n\nNext, create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs `scikit-learn` and `azureml-sdk`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nbikemodelenv = CondaDependencies()\nbikemodelenv.add_conda_package(\"scikit-learn\")\n\nwith open(\"bikemodelenv.yml\",\"w\") as f:\n    f.write(bikemodelenv.serialize_to_string())\n    \nprint('Complete')    ",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Complete\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Review the content of the `myenv.yml` file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(\"bikemodelenv.yml\",\"r\") as f:\n    print(f.read())\n    \nprint('Complete')    ",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "# Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n    # Required packages for AzureML execution, history, and data preparation.\n  - azureml-defaults\n\n- scikit-learn\nchannels:\n- conda-forge\n\nComplete\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create configuration file\n\nCreate a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"data\": \"biketype\",  \"method\" : \"sklearn\"}, \n                                               description='Predict biketype with sklearn')\n\nprint('Complete')",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Complete\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Deploy in ACI\nEstimated time to complete: **about 7-8 minutes**\n\nConfigure the image and deploy. The following code goes through these steps:\n\n1. Build an image using:\n   * The scoring file (`score.py`)\n   * The environment file (`myenv.yml`)\n   * The model file\n1. Register that image under the workspace. \n1. Send the image to the ACI container.\n1. Start up a container in ACI using the image.\n1. Get the web service HTTP endpoint."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.image import ContainerImage\n\n# configure the image\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"bikemodelenv.yml\")\n\nservice = Webservice.deploy_from_model(workspace=ws,\n                                       name='sklearn-biketype-svc2',\n                                       deployment_config=aciconfig,\n                                       models=[model],\n                                       image_config=image_config)\n\nservice.wait_for_deployment(show_output=True)\n\nprint('Complete')",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating image\nRunning.........................................\nSucceeded\nImage creation operation finished for image sklearn-biketype-svc2:1, operation \"Succeeded\"\nRunning..............\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\nComplete\nCPU times: user 1.47 s, sys: 551 ms, total: 2.02 s\nWall time: 5min 4s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(service.scoring_uri)",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": "http://cd816915-decc-4288-b078-bcd2c32667fe.westus.azurecontainer.io/score\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test deployed service\n\nEarlier you scored all the test data with the local version of the model. Now, you can test the deployed model by just passing the test data we used earlier, X_test.  We're doing this in the interest of time as it will serve us well just to test the deployed service.  In a real use case, you might want to specially prepare some input data.  \n\nThe following code goes through these steps:\n1. Send the data as a JSON array to the web service hosted in ACI. \n\n1. Use the SDK's `run` API to invoke the service. You can also make raw calls using any HTTP tool such as curl.\n\n1. Print the returned predictions and plot them along with the input images. Red font and inverse image (white on black) is used to highlight the misclassified samples. \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nX_train_all , X_test_all = train_test_split(df_features.values,test_size=0.2)       #test_size=0.5(whole_data)\nX_train_all[:,0]\ny_train = X_train_all[:,0]\ny_test = X_test_all[:,0]\nX_train_all.shape\nX_train = X_train_all[:,1:11]\nX_train.shape\nX_test = X_test_all[:,1:11]\nX_test\n\n",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 48,
          "data": {
            "text/plain": "array([[5, 1, 0, ..., 5, 1, 3],\n       [5, 0, 0, ..., 5, 1, 0],\n       [4, 1, 1, ..., 5, 1, 2],\n       ...,\n       [3, 0, 1, ..., 5, 1, 1],\n       [2, 1, 1, ..., 0, 1, 1],\n       [2, 1, 1, ..., 0, 0, 0]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_BikeTest = pd.read_csv('BikeFeatures.csv', sep=';')\ny_test = df_BikeTest['BikeTypeCat']\ndf_BikeTest.drop(columns=['BikeTypeCat'], inplace=True)\nX_test = df_BikeTest.values\nX_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\n\n\ntest_samples = json.dumps({\"data\": X_test.tolist()})\ntest_samples = bytes(test_samples, encoding='utf8')\n\n# predict using the deployed model\nresult = service.run(input_data=test_samples)\n\nprint('Complete')",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Complete\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Display the first few values to see that the call worked."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "result[0:5]",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": "[1, 0, 1, 1, 1]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also send raw HTTP request to test the web service.  Again, we are using the X_test data for convenience."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport json\n\n# send a random row from the test set to score\nrandom_index = np.random.randint(0, len(X_test)-1)\ninput_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n\n\nheaders = {'Content-Type':'application/json'}\n\n# for AKS deployment you'd need to the service key in the header as well\n# api_key = service.get_key()\n# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n\nresp = requests.post(service.scoring_uri, input_data, headers=headers)\n\nprint(\"POST to url\", service.scoring_uri)\n#print(\"input data:\", input_data)\nprint(\"label:\", y_test[random_index])\nprint(\"prediction:\", resp.text)\n\nprint('Complete')",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": "POST to url http://cd816915-decc-4288-b078-bcd2c32667fe.westus.azurecontainer.io/score\nlabel: 1\nprediction: [1]\nComplete\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Clean up resources\n\nTo keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Uncomment the line below and run the code to clean up the resources.\n\n# service.delete()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "roastala"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "msauthor": "roastala",
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}