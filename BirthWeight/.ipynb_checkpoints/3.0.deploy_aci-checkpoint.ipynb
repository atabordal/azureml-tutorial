{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bf571e",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bd34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.get(name='demo-aml',subscription_id='YOUR-SUSCRIPTION-ID',resource_group='demo-aml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6af9128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='demo-aml', subscription_id='92ba1152-3fff-41db-864f-d94ce03eb9e1', resource_group='demo-aml'), name=BABY-WEIGHT, id=BABY-WEIGHT:8, version=8, tags={'Model': 'BABY-WEIGHT', 'Type': 'Neural Network', 'User': 'alejandra.taborda-londono@capgemini.com'}, properties={})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List last model registered on ws in order to create the image\n",
    "from azureml.core import Model\n",
    "model_name = 'BABY-WEIGHT'\n",
    "list_models = Model.list(workspace=ws, name=model_name)\n",
    "model = list_models[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a383e",
   "metadata": {},
   "source": [
    "# Create inference configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72aff206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags=model.tags, \n",
    "                                               description=model.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a239054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "myenv = Environment.get(workspace=ws, name=\"keras-env\")\n",
    "inference_config = InferenceConfig(entry_script=\"./score.py\", environment=myenv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9b04e",
   "metadata": {},
   "source": [
    "# Deploy web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec16f519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-07-21 21:25:18+00:00 Creating Container Registry if not exists.\n",
      "2021-07-21 21:25:19+00:00 Registering the environment.\n",
      "2021-07-21 21:25:20+00:00 Use the existing image.\n",
      "2021-07-21 21:25:20+00:00 Generating deployment configuration.\n",
      "2021-07-21 21:25:21+00:00 Submitting deployment to compute.\n",
      "2021-07-21 21:25:24+00:00 Checking the status of deployment baby-weights-webservice-aci..\n",
      "2021-07-21 21:25:35+00:00 Checking the status of inference endpoint baby-weights-webservice-aci.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n",
      "2021-07-21T21:25:27,214783493+00:00 - gunicorn/run \n",
      "File not found: /var/azureml-app/.\n",
      "Starting HTTP server\n",
      "2021-07-21T21:25:27,215676198+00:00 - iot-server/run \n",
      "2021-07-21T21:25:27,218476212+00:00 - rsyslog/run \n",
      "2021-07-21T21:25:27,219450017+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-07-21T21:25:27,386938061+00:00 - iot-server/finish 1 0\n",
      "2021-07-21T21:25:27,388204167+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (12)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 42\n",
      "2021-07-21 21:25:28.424660: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_791654c3aef411ce39fc7aad12fa46a0/lib:/azureml-envs/azureml_791654c3aef411ce39fc7aad12fa46a0/lib:\n",
      "2021-07-21 21:25:28.424700: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-07-21 21:25:29,988 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2021-07-21 21:25:29,988 | root | INFO | Starting up request id generator\n",
      "2021-07-21 21:25:29,988 | root | INFO | Starting up app insight hooks\n",
      "2021-07-21 21:25:29,988 | root | INFO | Invoking user's init function\n",
      "2021-07-21 21:25:29.997843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_791654c3aef411ce39fc7aad12fa46a0/lib:/azureml-envs/azureml_791654c3aef411ce39fc7aad12fa46a0/lib:\n",
      "2021-07-21 21:25:29.997910: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-21 21:25:29.997934: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (wk-caas-3c537a51202f4e588b954e0b9e857779-0a70db6274d1ffa4c0640b): /proc/driver/nvidia/version does not exist\n",
      "2021-07-21 21:25:29.998185: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-21 21:25:30,110 | root | INFO | Users's init has completed successfully\n",
      "2021-07-21 21:25:30,112 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-07-21 21:25:30,112 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-07-21 21:25:30,113 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-07-21 21:25:35,283 | root | INFO | Swagger file not present\n",
      "2021-07-21 21:25:35,283 | root | INFO | 404\n",
      "127.0.0.1 - - [21/Jul/2021:21:25:35 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-07-21 21:25:40,401 | root | INFO | Swagger file not present\n",
      "2021-07-21 21:25:40,402 | root | INFO | 404\n",
      "127.0.0.1 - - [21/Jul/2021:21:25:40 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "service_name = 'baby-weights-webservice-aci' \n",
    "\n",
    "try:\n",
    "    Webservice.check_for_existing_webservice(name=service_name, workspace=ws)\n",
    "    service = Model.deploy(workspace=ws, \n",
    "                       name=service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "    print(service.state)\n",
    "    print(service.wait_for_deployment(show_output=True))\n",
    "\n",
    "except Exception as e: \n",
    "    #print(e)\n",
    "    service = Webservice(name=service_name, workspace=ws)\n",
    "    # Update to new model(s).\n",
    "    service.update(models=[model], inference_config=inference_config)\n",
    "    service.wait_for_deployment(show_output=True)\n",
    "    \n",
    "    print(service.state)\n",
    "    print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "print(service.state)\n",
    "print(service.wait_for_deployment(show_output=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16efeeb",
   "metadata": {},
   "source": [
    "## Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eace0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_val=pd.read_json('inputs/X_validation_data.json', orient=\"split\")\n",
    "X_val.head()\n",
    "\n",
    "X_val = X_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e353101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST to url http://4d3373d3-979c-473a-b9b1-22d9458183d2.westeurope.azurecontainer.io/score\n",
      "input data: {\"data\": [[1.0279457598, 0.41701790990000004, 0.34644487620000003, 0.1509989142, -0.1235973895, -0.9249775915, -0.10685439490000001, -0.4628319709, 1.3865730038000001, 0.9039395828000001, -0.33874908130000003, -0.3531535229, -0.2438582988, -0.026721312400000002, -0.1921349527, -0.0653348063, -0.10667954090000001, 5.4075479387, -0.1140049685, -0.1225357703, -0.0414599321, -0.11815008090000001, -0.22951098050000002, -0.061816280200000004, -0.0612510832, -0.0755838971, -0.0945258989, -0.046841785600000005, -0.0615746786, -0.0587682023]]}\n",
      "prediction: [[3.5170540809631348]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Online prediction: send a random row from the validatition set to score\n",
    "random_index = np.random.randint(0, len(X_val)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_val[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "print(\"input data:\", input_data)\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9d1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
