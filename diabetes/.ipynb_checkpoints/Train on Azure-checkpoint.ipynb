{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #1: Train a ML model on Azure Machine Learning service.\n",
    "\n",
    "In this lab, you train the classifcation machine learning model on remote compute resources. You'll use the training and deployment workflow for Azure Machine Learning service (preview) in a Python Jupyter notebook.  \n",
    "\n",
    "Learn how to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Access and examine the data\n",
    "> * Train a simple classification model on a remote cluster\n",
    "> * Review training results, find and register the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Create an Experiment in an existing Workspace.\n",
    "* Configure AutoML using AutoMLConfig.\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace Properties from Above:\n",
    "\n",
    "Name | Description\n",
    "---- | -----------\n",
    "name            | A name you chose to call the workspace.  We'll use the value aready in the code.\n",
    "subscription_id | The id of the subscription the workspace will be assigned to.  You can get this from the Azure portal.\n",
    "resource_group  | A name you want all the Azure resourced creates for the workspace to be associated with.  Makes rsource management easier.\n",
    "location | Azure data center location closest to you that support creation of AMLS workspaces.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AMLS workspace...\n",
    "\n",
    "You will be asked to log into Azure and be given a code in the output message area to enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying AppInsights with name docswsinsights1bf2023985.\n",
      "Deployed AppInsights with name docswsinsights1bf2023985. Took 3.89 seconds.\n",
      "Deploying KeyVault with name docswskeyvault46292f87e3.\n",
      "Deploying StorageAccount with name docswsstorage97b0140e1cd.\n",
      "Deployed KeyVault with name docswskeyvault46292f87e3. Took 20.02 seconds.\n",
      "Deployed StorageAccount with name docswsstorage97b0140e1cd. Took 23.36 seconds.\n",
      "Deploying Workspace with name docs-ws.\n",
      "Deployed Workspace with name docs-ws. Took 40.82 seconds.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='docs-ws',\n",
    "            subscription_id='c958680c-dc7a-403c-bb83-74f48dce46b3', \n",
    "            resource_group='docs-aml',\n",
    "            create_resource_group = True,\n",
    "            location='West US'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got a Azure Machine Learning Workspace please use the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.get(name='docs-ws',\n",
    "            subscription_id='c958680c-dc7a-403c-bb83-74f48dce46b3', \n",
    "            resource_group='docs-aml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace in future work...\n",
    "\n",
    "In the future, we can use the code below to connect back to this workspace. \n",
    "The code creates a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`.  You don't need to use this now since we are still connected from when we created the workspace but this will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved.\n"
     ]
    }
   ],
   "source": [
    "# Create the configuration file.\n",
    "ws.write_config(path='.', file_name='config.json')\n",
    "print('Configuration saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'sklearn-diabetes'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "create mlc",
     "amlcompute"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new compute target...\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-01-25T14:56:19.881000+00:00', 'errors': None, 'creationTime': '2021-01-25T14:56:09.991965+00:00', 'modifiedTime': '2021-01-25T14:56:27.529574+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the necessary packages and compute resources to train a model in the cloud. \n",
    "\n",
    "## Verify you have the data\n",
    "\n",
    "You already explored the data in the last lab.  You need to copy the data into the cloud so it can be accessed by your cloud training environment.  We saved the model training data to a csv file so all we have to do is load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a few rows of data to make sure the load worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_diabetes = pd.read_csv('inputs/diabetes.csv')\n",
    "df_diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the df_feeatures one more time...\n",
    "df_diabetes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory\n",
    "Create a directory to deliver the necessary code from your computer to the remote resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About training scripts...\n",
    "\n",
    "To train a model in an Azure container, we need to get the model training script to the container. We start by saving the model training script to a Python script file, i.e. .py.  This will be uploaded to the Azure container later.  We don't need any exploratory code in this script, just what is needed to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/atabordal/azureml-tutorial/diabetes/sklearn-mnist/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from azureml.core import Run\n",
    "print('Libraries Imported')\n",
    "\n",
    "# ***  Azure Machine Learning service specfic code starts... ***\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--penalty', type=str, dest='penalty', default='l2', help='penalty')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "data_folder = args.data_folder\n",
    "penalty = args.penalty\n",
    "\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# ***  Azure Machine Learning service specfic code ends. ***\n",
    "\n",
    "filepath = os.path.join(data_folder, 'diabetes.csv')\n",
    "\n",
    "df_diabetes = pd.read_csv(filepath)\n",
    "#Features data\n",
    "X0= df_diabetes.loc[:,  df_diabetes.columns != 'Outcome']\n",
    "#label data\n",
    "y= df_diabetes[['Outcome']]\n",
    "\n",
    "# Scaler the data \n",
    "names = X0.columns\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X0)\n",
    "X = pd.DataFrame(X, columns=names)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, test_size=0.3)\n",
    "\n",
    "# Adjuting model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=penalty,random_state=0)\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "print('Regressionn Model Trained.')\n",
    "\n",
    "# Predict using the test data...\n",
    "print('Running the test dataset through...')\n",
    "y_predtest = clf.predict(X_test)\n",
    "print('Test dataset scored.')\n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "acc= clf.score(X_test, y_test)\n",
    "print(\"accuracy = \",acc * 100,\"%\")\n",
    "\n",
    "# ***  Azure Machine Learning service specfic code starts... ***\n",
    "run.log('data_dir', data_folder)\n",
    "run.log('accuracy', np.float(acc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/diabetes_model.pkl')\n",
    "X_validate.to_json('outputs/validation_data.json', orient=\"split\")\n",
    "\n",
    "# ***  Azure Machine Learning service specfic code ends. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the script gets data and saves models:\n",
    "\n",
    "+ The training script reads an argument to find the directory containing the data.  When you submit the job later, you point to the datastore for this argument:\n",
    "`parser.add_argument('--data-folder', type=str, dest='data_folder', help='data directory mounting point')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ The training script saves your model into a directory named outputs. <br/>\n",
    "`joblib.dump(value=clf, filename='outputs/diabetes_model.pkl')`<br/>\n",
    "Anything written in this directory is automatically uploaded into your workspace. You'll access your model from this directory later in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Configure the ScriptRunConfig by specifying:\n",
    "\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the AmlCompute you created\n",
    "* The training script name, train.py\n",
    "* An environment that contains the libraries needed to run the script\n",
    "* Arguments required from the training script. \n",
    "\n",
    "In this tutorial, the target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the dataset.\n",
    "\n",
    "First, create the environment that contains: the scikit-learn library, azureml-dataset-runtime required for accessing the dataset, and azureml-defaults which contains the dependencies for logging metrics. The azureml-defaults also contains the dependencies required for deploying the model as a web service later in the part 2 of the tutorial.\n",
    "\n",
    "Once the environment is defined, register it with the Workspace to re-use it in part 2 of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210104.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"tutorial-env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6.2\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-dataset-runtime[pandas,fuse]~=1.20.0\",\n",
       "                        \"azureml-defaults~=1.20.0\"\n",
       "                    ]\n",
       "                },\n",
       "                \"scikit-learn==0.22.1\"\n",
       "            ],\n",
       "            \"name\": \"azureml_55a8b2f65be1eead47aa6b710287793f\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"1\"\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create the ScriptRunConfig by specifying the training script, compute target and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', 'inputs', '--penalty', 'l2']\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "Run the experiment by submitting the ScriptRunConfig object. And you can navigate to Azure portal to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>sklearn-diabetes</td><td>sklearn-diabetes_1611586822_15a97156</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/experiments/sklearn-diabetes/runs/sklearn-diabetes_1611586822_15a97156?wsid=/subscriptions/c958680c-dc7a-403c-bb83-74f48dce46b3/resourcegroups/docs-aml/workspaces/docs-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: sklearn-diabetes,\n",
       "Id: sklearn-diabetes_1611586822_15a97156,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the call is asynchronous, it returns a Preparing or Running state as soon as the job is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies in the Azure ML environment don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the Azure ML environment. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab4da8df08448849d4e2630f42f8c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "\"ServiceException:\\n\\tCode: 404\\n\\tMessage: (UserError) Arm resource not found\\n\\tDetails:\\n\\n\\tHeaders: {\\n\\t    \\\"Date\\\": \\\"Mon, 25 Jan 2021 15:39:13 GMT\\\",\\n\\t    \\\"Content-Type\\\": \\\"application/json; charset=utf-8\\\",\\n\\t    \\\"Transfer-Encoding\\\": \\\"chunked\\\",\\n\\t    \\\"Connection\\\": \\\"keep-alive\\\",\\n\\t    \\\"Vary\\\": \\\"Accept-Encoding\\\",\\n\\t    \\\"Request-Context\\\": \\\"appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d\\\",\\n\\t    \\\"x-ms-response-type\\\": \\\"error\\\",\\n\\t    \\\"X-Content-Type-Options\\\": \\\"nosniff\\\",\\n\\t    \\\"x-request-time\\\": \\\"0.078\\\",\\n\\t    \\\"Strict-Transport-Security\\\": \\\"max-age=15724800; includeSubDomains; preload\\\",\\n\\t    \\\"Content-Encoding\\\": \\\"gzip\\\"\\n\\t}\\n\\tInnerException: {\\n    \\\"additional_properties\\\": {},\\n    \\\"error\\\": {\\n        \\\"additional_properties\\\": {\\n            \\\"debugInfo\\\": null\\n        },\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"severity\\\": null,\\n        \\\"message\\\": \\\"Arm resource not found\\\",\\n        \\\"message_format\\\": null,\\n        \\\"message_parameters\\\": null,\\n        \\\"reference_code\\\": null,\\n        \\\"details_uri\\\": null,\\n        \\\"target\\\": null,\\n        \\\"details\\\": [],\\n        \\\"inner_error\\\": {\\n            \\\"additional_properties\\\": {},\\n            \\\"code\\\": \\\"NotFoundError\\\",\\n            \\\"inner_error\\\": null\\n        }\\n    },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": \\\"894fe06894313345bea07a2d5671e7a2\\\",\\n        \\\"request\\\": \\\"8a8db13074f0254e\\\"\\n    },\\n    \\\"environment\\\": \\\"westus\\\",\\n    \\\"location\\\": \\\"westus\\\",\\n    \\\"time\\\": {},\\n    \\\"component_name\\\": \\\"run-history\\\"\\n}\""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion\n",
    "\n",
    "Model training happens in the background. You can use `wait_for_completion` to block and wait until the model has completed training before running more code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify show_output to True for a verbose log\n",
    "run.wait_for_completion(show_output=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "You now have a model trained on a remote cluster.  Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next tutorial you will explore this model in more detail.\n",
    "\n",
    "## Register model\n",
    "\n",
    "The last step in the training script wrote the file `outputs/diabetes_model.pkl` in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n",
    "\n",
    "You can see files associated with that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model \n",
    "model_name = \"sklearn_diabetes\"\n",
    "model = Model.register(model_path=\"outputs/diabetes_model.pkl\",\n",
    "                        model_name=model_name,\n",
    "                        tags={\"data\": \"diabetes\", \"model\": \"regression\"},\n",
    "                        description=\"Ridge regression model to predict diabetes\",\n",
    "                        workspace=ws)\n",
    "\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "msauthor": "roastala"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
